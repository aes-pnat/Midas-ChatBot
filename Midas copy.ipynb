{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apasalic/anaconda/envs/nlp_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-29 00:45:14.651392: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinanceInc/finbert-pretrain\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"FinanceInc/finbert-pretrain\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user report                            total submissions   10   first seen in wsb   2 years ago   total comments   332   previous best dd      account age   3 years   scan  comment     submission   \n"
     ]
    }
   ],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words and lemmatize the words\n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return text\n",
    "\n",
    "input_text = '\\n**User Report**| | | |\\n:--|:--|:--|:--\\n**Total Submissions**|10|**First Seen In WSB**|2 years ago\\n**Total Comments**|332|**Previous Best DD**|\\n**Account Age**|3 years|[^scan ^comment ](https://www.reddit.com/message/compose/?to=VisualMod&subject=scan_comment&message=Replace%20this%20text%20with%20a%20comment%20ID%20(which%20looks%20like%20h26cq3k\\\\)%20to%20have%20the%20bot%20scan%20your%20comment%20and%20correct%20your%20first%20seen%20date.)|[^scan ^submission ](https://www.reddit.com/message/compose/?to=VisualMod&subject=scan_submission&message=Replace%20this%20text%20with%20a%20submission%20ID%20(which%20looks%20like%20h26cq3k\\\\)%20to%20have%20the%20bot%20scan%20your%20submission%20and%20correct%20your%20first%20seen%20date.)'\n",
    "preprocessed_text = preprocess_text(input_text)\n",
    "print(preprocessed_text)\n",
    "\n",
    "def decontractions(phrase):\n",
    "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my  o position  am i fuk d  i have a severe ad...</td>\n",
       "      <td>yeah you should sell it all now and buy back a...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what s up with market too much red and blood s...</td>\n",
       "      <td>welcome to r dividends   if you are new to the...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earnings drop considerably during or after cen...</td>\n",
       "      <td>forward eps instead of actual  skip</td>\n",
       "      <td>stockmarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saudi arabia and opec  makes surprise 1 millio...</td>\n",
       "      <td>when we really look at the amount petroleum pr...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wework saga cost masayoshi son  11 5 billion a...</td>\n",
       "      <td>wow  they sold nvidia at the bottom in jan 201...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41008</th>\n",
       "      <td>investment hindsight lessons from a 50 year ol...</td>\n",
       "      <td>i m going to add one more for the young bucks ...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41009</th>\n",
       "      <td>getting closer to my second goal of  100 a mon...</td>\n",
       "      <td>how old are you  you re likely missing put on ...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41010</th>\n",
       "      <td>got my dividend portfolio to   10 a month   i ...</td>\n",
       "      <td>as a 30 year old i m kicking myself for not be...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41011</th>\n",
       "      <td>introducing  wsb s first ever paper trading co...</td>\n",
       "      <td>beta</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41012</th>\n",
       "      <td>mysterious pneumonia outbreak  overwhelms chin...</td>\n",
       "      <td>covid 23  so puts in like 5 months followed by...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41013 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               questions  \\\n",
       "0      my  o position  am i fuk d  i have a severe ad...   \n",
       "1      what s up with market too much red and blood s...   \n",
       "2      earnings drop considerably during or after cen...   \n",
       "3      saudi arabia and opec  makes surprise 1 millio...   \n",
       "4      wework saga cost masayoshi son  11 5 billion a...   \n",
       "...                                                  ...   \n",
       "41008  investment hindsight lessons from a 50 year ol...   \n",
       "41009  getting closer to my second goal of  100 a mon...   \n",
       "41010  got my dividend portfolio to   10 a month   i ...   \n",
       "41011  introducing  wsb s first ever paper trading co...   \n",
       "41012  mysterious pneumonia outbreak  overwhelms chin...   \n",
       "\n",
       "                                                 answers            tags  \n",
       "0      yeah you should sell it all now and buy back a...       dividends  \n",
       "1      welcome to r dividends   if you are new to the...       dividends  \n",
       "2                   forward eps instead of actual  skip      stockmarket  \n",
       "3      when we really look at the amount petroleum pr...          stocks  \n",
       "4      wow  they sold nvidia at the bottom in jan 201...          stocks  \n",
       "...                                                  ...             ...  \n",
       "41008  i m going to add one more for the young bucks ...       dividends  \n",
       "41009  how old are you  you re likely missing put on ...       dividends  \n",
       "41010  as a 30 year old i m kicking myself for not be...       dividends  \n",
       "41011                                               beta  wallstreetbets  \n",
       "41012  covid 23  so puts in like 5 months followed by...  wallstreetbets  \n",
       "\n",
       "[41013 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df_reddit = pd.read_csv('qna_df_reddit.csv')\n",
    "qna_df_reddit = qna_df_reddit.sample(frac=1).reset_index(drop=True)\n",
    "qna_df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_df_reddit = qna_df_reddit.map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my  o position  am i fuk d  i have a severe ad...</td>\n",
       "      <td>yeah you should sell it all now and buy back a...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what s up with market too much red and blood s...</td>\n",
       "      <td>welcome to r dividends   if you are new to the...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earnings drop considerably during or after cen...</td>\n",
       "      <td>forward eps instead of actual  skip</td>\n",
       "      <td>stockmarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saudi arabia and opec  makes surprise 1 millio...</td>\n",
       "      <td>when we really look at the amount petroleum pr...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wework saga cost masayoshi son  11 5 billion a...</td>\n",
       "      <td>wow  they sold nvidia at the bottom in jan 201...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40801</th>\n",
       "      <td>investment hindsight lessons from a 50 year ol...</td>\n",
       "      <td>i m going to add one more for the young bucks ...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40802</th>\n",
       "      <td>getting closer to my second goal of  100 a mon...</td>\n",
       "      <td>how old are you  you re likely missing put on ...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40803</th>\n",
       "      <td>got my dividend portfolio to   10 a month   i ...</td>\n",
       "      <td>as a 30 year old i m kicking myself for not be...</td>\n",
       "      <td>dividends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40804</th>\n",
       "      <td>introducing  wsb s first ever paper trading co...</td>\n",
       "      <td>beta</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40805</th>\n",
       "      <td>mysterious pneumonia outbreak  overwhelms chin...</td>\n",
       "      <td>covid 23  so puts in like 5 months followed by...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40806 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               questions  \\\n",
       "0      my  o position  am i fuk d  i have a severe ad...   \n",
       "1      what s up with market too much red and blood s...   \n",
       "2      earnings drop considerably during or after cen...   \n",
       "3      saudi arabia and opec  makes surprise 1 millio...   \n",
       "4      wework saga cost masayoshi son  11 5 billion a...   \n",
       "...                                                  ...   \n",
       "40801  investment hindsight lessons from a 50 year ol...   \n",
       "40802  getting closer to my second goal of  100 a mon...   \n",
       "40803  got my dividend portfolio to   10 a month   i ...   \n",
       "40804  introducing  wsb s first ever paper trading co...   \n",
       "40805  mysterious pneumonia outbreak  overwhelms chin...   \n",
       "\n",
       "                                                 answers            tags  \n",
       "0      yeah you should sell it all now and buy back a...       dividends  \n",
       "1      welcome to r dividends   if you are new to the...       dividends  \n",
       "2                   forward eps instead of actual  skip      stockmarket  \n",
       "3      when we really look at the amount petroleum pr...          stocks  \n",
       "4      wow  they sold nvidia at the bottom in jan 201...          stocks  \n",
       "...                                                  ...             ...  \n",
       "40801  i m going to add one more for the young bucks ...       dividends  \n",
       "40802  how old are you  you re likely missing put on ...       dividends  \n",
       "40803  as a 30 year old i m kicking myself for not be...       dividends  \n",
       "40804                                               beta  wallstreetbets  \n",
       "40805  covid 23  so puts in like 5 months followed by...  wallstreetbets  \n",
       "\n",
       "[40806 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df_reddit['questions'] = qna_df_reddit['questions'].apply(lambda x: np.nan if re.match(r'^\\s*$', x) else x)\n",
    "qna_df_reddit['answers'] = qna_df_reddit['answers'].apply(lambda x: np.nan if re.match(r'^\\s*$', x) else x)\n",
    "\n",
    "qna_df_reddit = qna_df_reddit.dropna().reset_index(drop=True)\n",
    "qna_df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_df_reddit['questions'] = qna_df_reddit['questions'].apply(lambda x: preprocess_text(x))\n",
    "qna_df_reddit['answers'] = qna_df_reddit['answers'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "qna_df_reddit['questions'] = qna_df_reddit['questions'].apply(lambda x: decontractions(x))\n",
    "qna_df_reddit['answers'] = qna_df_reddit['answers'].apply(lambda x: decontractions(x))\n",
    "\n",
    "qna_df_reddit['question_tokens'] = qna_df_reddit['questions'].apply(lambda x: finbert_tokenizer.tokenize(x))\n",
    "qna_df_reddit['answer_tokens'] = qna_df_reddit['answers'].apply(lambda x: finbert_tokenizer.tokenize(x))\n",
    "\n",
    "qna_df_reddit['question_tokens'] = qna_df_reddit['question_tokens'].apply(lambda x: ['[CLS]'] + x + ['[SEP]'])\n",
    "qna_df_reddit['answer_tokens'] = qna_df_reddit['answer_tokens'].apply(lambda x: ['[CLS]'] + x + ['[SEP]'])\n",
    "\n",
    "qna_df_reddit['question_ids'] = qna_df_reddit['question_tokens'].apply(lambda x: finbert_tokenizer.convert_tokens_to_ids(x))\n",
    "qna_df_reddit['answer_ids'] = qna_df_reddit['answer_tokens'].apply(lambda x: finbert_tokenizer.convert_tokens_to_ids(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>tags</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>answer_tokens</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>answer_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my  o position  am i fuk d  i have a severe ad...</td>\n",
       "      <td>yeah you should sell it all now and buy back a...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], my, o, position, am, i, fu, ##k, d, i,...</td>\n",
       "      <td>[[CLS], yeah, you, should, sell, it, all, now,...</td>\n",
       "      <td>[3, 657, 1506, 497, 1198, 44, 9841, 994, 1514,...</td>\n",
       "      <td>[3, 15471, 40, 144, 329, 41, 69, 212, 8, 500, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what s up with market too much red and blood s...</td>\n",
       "      <td>welcome to r dividends   if you are new to the...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], what, s, up, with, market, too, much, ...</td>\n",
       "      <td>[[CLS], welcome, to, r, dividends, if, you, ar...</td>\n",
       "      <td>[3, 163, 58, 129, 20, 52, 1727, 406, 2932, 8, ...</td>\n",
       "      <td>[3, 2133, 9, 2859, 751, 60, 40, 21, 56, 9, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earnings drop considerably during or after cen...</td>\n",
       "      <td>forward eps instead of actual  skip</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], earnings, drop, considerably, during, ...</td>\n",
       "      <td>[[CLS], forward, eps, instead, of, actual, ski...</td>\n",
       "      <td>[3, 149, 2535, 6800, 84, 16, 293, 2034, 230, 7...</td>\n",
       "      <td>[3, 663, 249, 3592, 7, 499, 26189, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saudi arabia and opec  makes surprise 1 millio...</td>\n",
       "      <td>when we really look at the amount petroleum pr...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>[[CLS], saudi, arabia, and, opec, makes, surpr...</td>\n",
       "      <td>[[CLS], when, we, really, look, at, the, amoun...</td>\n",
       "      <td>[3, 4807, 6569, 8, 25140, 1221, 5544, 428, 27,...</td>\n",
       "      <td>[3, 181, 13, 392, 368, 28, 6, 224, 3360, 89, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wework saga cost masayoshi son  11 5 billion a...</td>\n",
       "      <td>wow  they sold nvidia at the bottom in jan 201...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>[[CLS], we, ##work, sag, ##a, cost, mas, ##ay,...</td>\n",
       "      <td>[[CLS], wo, ##w, they, sold, nvidia, at, the, ...</td>\n",
       "      <td>[3, 13, 8724, 21481, 363, 121, 13100, 4345, 29...</td>\n",
       "      <td>[3, 9566, 1652, 174, 449, 10627, 28, 6, 2169, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40801</th>\n",
       "      <td>investment hindsight lessons from a 50 year ol...</td>\n",
       "      <td>i m going to add one more for the young bucks ...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], investment, hind, ##sight, lesson, ##s...</td>\n",
       "      <td>[[CLS], i, m, going, to, add, one, more, for, ...</td>\n",
       "      <td>[3, 79, 29270, 16560, 23379, 63, 23, 11, 1104,...</td>\n",
       "      <td>[3, 44, 1276, 201, 9, 1122, 137, 59, 14, 6, 66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40802</th>\n",
       "      <td>getting closer to my second goal of  100 a mon...</td>\n",
       "      <td>how old are you  you re likely missing put on ...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], getting, closer, to, my, second, goal,...</td>\n",
       "      <td>[[CLS], how, old, are, you, you, re, likely, m...</td>\n",
       "      <td>[3, 1328, 3158, 9, 657, 180, 1937, 7, 1081, 11...</td>\n",
       "      <td>[3, 283, 2664, 21, 40, 40, 482, 419, 11377, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40803</th>\n",
       "      <td>got my dividend portfolio to   10 a month   i ...</td>\n",
       "      <td>as a 30 year old i m kicking myself for not be...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], got, my, dividend, portfolio, to, 10, ...</td>\n",
       "      <td>[[CLS], as, a, 30, year, old, i, m, kick, ##in...</td>\n",
       "      <td>[3, 831, 657, 699, 318, 9, 685, 11, 804, 44, 1...</td>\n",
       "      <td>[3, 18, 11, 1000, 62, 2664, 44, 1276, 7974, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40804</th>\n",
       "      <td>introducing  wsb s first ever paper trading co...</td>\n",
       "      <td>beta</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], introducing, ws, ##b, s, first, ever, ...</td>\n",
       "      <td>[[CLS], beta, [SEP]]</td>\n",
       "      <td>[3, 5638, 7765, 974, 58, 78, 3157, 1659, 429, ...</td>\n",
       "      <td>[3, 5457, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40805</th>\n",
       "      <td>mysterious pneumonia outbreak  overwhelms chin...</td>\n",
       "      <td>covid 23  so puts in like 5 months followed by...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], mys, ##teri, ##ous, pneumo, ##nia, out...</td>\n",
       "      <td>[[CLS], cov, ##id, 23, so, puts, in, like, 5, ...</td>\n",
       "      <td>[3, 22940, 12344, 3837, 21721, 9117, 16267, 76...</td>\n",
       "      <td>[3, 17101, 3531, 2249, 96, 5960, 10, 266, 898,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40806 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               questions  \\\n",
       "0      my  o position  am i fuk d  i have a severe ad...   \n",
       "1      what s up with market too much red and blood s...   \n",
       "2      earnings drop considerably during or after cen...   \n",
       "3      saudi arabia and opec  makes surprise 1 millio...   \n",
       "4      wework saga cost masayoshi son  11 5 billion a...   \n",
       "...                                                  ...   \n",
       "40801  investment hindsight lessons from a 50 year ol...   \n",
       "40802  getting closer to my second goal of  100 a mon...   \n",
       "40803  got my dividend portfolio to   10 a month   i ...   \n",
       "40804  introducing  wsb s first ever paper trading co...   \n",
       "40805  mysterious pneumonia outbreak  overwhelms chin...   \n",
       "\n",
       "                                                 answers            tags  \\\n",
       "0      yeah you should sell it all now and buy back a...       dividends   \n",
       "1      welcome to r dividends   if you are new to the...       dividends   \n",
       "2                   forward eps instead of actual  skip      stockmarket   \n",
       "3      when we really look at the amount petroleum pr...          stocks   \n",
       "4      wow  they sold nvidia at the bottom in jan 201...          stocks   \n",
       "...                                                  ...             ...   \n",
       "40801  i m going to add one more for the young bucks ...       dividends   \n",
       "40802  how old are you  you re likely missing put on ...       dividends   \n",
       "40803  as a 30 year old i m kicking myself for not be...       dividends   \n",
       "40804                                               beta  wallstreetbets   \n",
       "40805  covid 23  so puts in like 5 months followed by...  wallstreetbets   \n",
       "\n",
       "                                         question_tokens  \\\n",
       "0      [[CLS], my, o, position, am, i, fu, ##k, d, i,...   \n",
       "1      [[CLS], what, s, up, with, market, too, much, ...   \n",
       "2      [[CLS], earnings, drop, considerably, during, ...   \n",
       "3      [[CLS], saudi, arabia, and, opec, makes, surpr...   \n",
       "4      [[CLS], we, ##work, sag, ##a, cost, mas, ##ay,...   \n",
       "...                                                  ...   \n",
       "40801  [[CLS], investment, hind, ##sight, lesson, ##s...   \n",
       "40802  [[CLS], getting, closer, to, my, second, goal,...   \n",
       "40803  [[CLS], got, my, dividend, portfolio, to, 10, ...   \n",
       "40804  [[CLS], introducing, ws, ##b, s, first, ever, ...   \n",
       "40805  [[CLS], mys, ##teri, ##ous, pneumo, ##nia, out...   \n",
       "\n",
       "                                           answer_tokens  \\\n",
       "0      [[CLS], yeah, you, should, sell, it, all, now,...   \n",
       "1      [[CLS], welcome, to, r, dividends, if, you, ar...   \n",
       "2      [[CLS], forward, eps, instead, of, actual, ski...   \n",
       "3      [[CLS], when, we, really, look, at, the, amoun...   \n",
       "4      [[CLS], wo, ##w, they, sold, nvidia, at, the, ...   \n",
       "...                                                  ...   \n",
       "40801  [[CLS], i, m, going, to, add, one, more, for, ...   \n",
       "40802  [[CLS], how, old, are, you, you, re, likely, m...   \n",
       "40803  [[CLS], as, a, 30, year, old, i, m, kick, ##in...   \n",
       "40804                               [[CLS], beta, [SEP]]   \n",
       "40805  [[CLS], cov, ##id, 23, so, puts, in, like, 5, ...   \n",
       "\n",
       "                                            question_ids  \\\n",
       "0      [3, 657, 1506, 497, 1198, 44, 9841, 994, 1514,...   \n",
       "1      [3, 163, 58, 129, 20, 52, 1727, 406, 2932, 8, ...   \n",
       "2      [3, 149, 2535, 6800, 84, 16, 293, 2034, 230, 7...   \n",
       "3      [3, 4807, 6569, 8, 25140, 1221, 5544, 428, 27,...   \n",
       "4      [3, 13, 8724, 21481, 363, 121, 13100, 4345, 29...   \n",
       "...                                                  ...   \n",
       "40801  [3, 79, 29270, 16560, 23379, 63, 23, 11, 1104,...   \n",
       "40802  [3, 1328, 3158, 9, 657, 180, 1937, 7, 1081, 11...   \n",
       "40803  [3, 831, 657, 699, 318, 9, 685, 11, 804, 44, 1...   \n",
       "40804  [3, 5638, 7765, 974, 58, 78, 3157, 1659, 429, ...   \n",
       "40805  [3, 22940, 12344, 3837, 21721, 9117, 16267, 76...   \n",
       "\n",
       "                                              answer_ids  \n",
       "0      [3, 15471, 40, 144, 329, 41, 69, 212, 8, 500, ...  \n",
       "1      [3, 2133, 9, 2859, 751, 60, 40, 21, 56, 9, 6, ...  \n",
       "2                  [3, 663, 249, 3592, 7, 499, 26189, 4]  \n",
       "3      [3, 181, 13, 392, 368, 28, 6, 224, 3360, 89, 1...  \n",
       "4      [3, 9566, 1652, 174, 449, 10627, 28, 6, 2169, ...  \n",
       "...                                                  ...  \n",
       "40801  [3, 44, 1276, 201, 9, 1122, 137, 59, 14, 6, 66...  \n",
       "40802  [3, 283, 2664, 21, 40, 40, 482, 419, 11377, 10...  \n",
       "40803  [3, 18, 11, 1000, 62, 2664, 44, 1276, 7974, 22...  \n",
       "40804                                       [3, 5457, 4]  \n",
       "40805  [3, 17101, 3531, 2249, 96, 5960, 10, 266, 898,...  \n",
       "\n",
       "[40806 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max question length:  5951\n"
     ]
    }
   ],
   "source": [
    "max_question_len = qna_df_reddit['question_ids'].apply(len).max()\n",
    "max_answer_len = qna_df_reddit['answer_ids'].apply(len).max()\n",
    "MAX_LEN = max(max_question_len, max_answer_len)\n",
    "\n",
    "print('Max question length: ', MAX_LEN)\n",
    "\n",
    "qna_df_reddit['question_ids_pad'] = qna_df_reddit['question_ids'].apply(lambda x: tf.keras.preprocessing.sequence.pad_sequences([x], maxlen=MAX_LEN, padding='post', truncating='post')[0])\n",
    "qna_df_reddit['answer_ids_pad'] = qna_df_reddit['answer_ids'].apply(lambda x: tf.keras.preprocessing.sequence.pad_sequences([x], maxlen=MAX_LEN, padding='post', truncating='post')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>tags</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>answer_tokens</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>answer_ids</th>\n",
       "      <th>question_ids_pad</th>\n",
       "      <th>answer_ids_pad</th>\n",
       "      <th>question_attention_mask</th>\n",
       "      <th>answer_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my  o position  am i fuk d  i have a severe ad...</td>\n",
       "      <td>yeah you should sell it all now and buy back a...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], my, o, position, am, i, fu, ##k, d, i,...</td>\n",
       "      <td>[[CLS], yeah, you, should, sell, it, all, now,...</td>\n",
       "      <td>[3, 657, 1506, 497, 1198, 44, 9841, 994, 1514,...</td>\n",
       "      <td>[3, 15471, 40, 144, 329, 41, 69, 212, 8, 500, ...</td>\n",
       "      <td>[3, 657, 1506, 497, 1198, 44, 9841, 994, 1514,...</td>\n",
       "      <td>[3, 15471, 40, 144, 329, 41, 69, 212, 8, 500, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what s up with market too much red and blood s...</td>\n",
       "      <td>welcome to r dividends   if you are new to the...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], what, s, up, with, market, too, much, ...</td>\n",
       "      <td>[[CLS], welcome, to, r, dividends, if, you, ar...</td>\n",
       "      <td>[3, 163, 58, 129, 20, 52, 1727, 406, 2932, 8, ...</td>\n",
       "      <td>[3, 2133, 9, 2859, 751, 60, 40, 21, 56, 9, 6, ...</td>\n",
       "      <td>[3, 163, 58, 129, 20, 52, 1727, 406, 2932, 8, ...</td>\n",
       "      <td>[3, 2133, 9, 2859, 751, 60, 40, 21, 56, 9, 6, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earnings drop considerably during or after cen...</td>\n",
       "      <td>forward eps instead of actual  skip</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], earnings, drop, considerably, during, ...</td>\n",
       "      <td>[[CLS], forward, eps, instead, of, actual, ski...</td>\n",
       "      <td>[3, 149, 2535, 6800, 84, 16, 293, 2034, 230, 7...</td>\n",
       "      <td>[3, 663, 249, 3592, 7, 499, 26189, 4]</td>\n",
       "      <td>[3, 149, 2535, 6800, 84, 16, 293, 2034, 230, 7...</td>\n",
       "      <td>[3, 663, 249, 3592, 7, 499, 26189, 4, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saudi arabia and opec  makes surprise 1 millio...</td>\n",
       "      <td>when we really look at the amount petroleum pr...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>[[CLS], saudi, arabia, and, opec, makes, surpr...</td>\n",
       "      <td>[[CLS], when, we, really, look, at, the, amoun...</td>\n",
       "      <td>[3, 4807, 6569, 8, 25140, 1221, 5544, 428, 27,...</td>\n",
       "      <td>[3, 181, 13, 392, 368, 28, 6, 224, 3360, 89, 1...</td>\n",
       "      <td>[3, 4807, 6569, 8, 25140, 1221, 5544, 428, 27,...</td>\n",
       "      <td>[3, 181, 13, 392, 368, 28, 6, 224, 3360, 89, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wework saga cost masayoshi son  11 5 billion a...</td>\n",
       "      <td>wow  they sold nvidia at the bottom in jan 201...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>[[CLS], we, ##work, sag, ##a, cost, mas, ##ay,...</td>\n",
       "      <td>[[CLS], wo, ##w, they, sold, nvidia, at, the, ...</td>\n",
       "      <td>[3, 13, 8724, 21481, 363, 121, 13100, 4345, 29...</td>\n",
       "      <td>[3, 9566, 1652, 174, 449, 10627, 28, 6, 2169, ...</td>\n",
       "      <td>[3, 13, 8724, 21481, 363, 121, 13100, 4345, 29...</td>\n",
       "      <td>[3, 9566, 1652, 174, 449, 10627, 28, 6, 2169, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40801</th>\n",
       "      <td>investment hindsight lessons from a 50 year ol...</td>\n",
       "      <td>i m going to add one more for the young bucks ...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], investment, hind, ##sight, lesson, ##s...</td>\n",
       "      <td>[[CLS], i, m, going, to, add, one, more, for, ...</td>\n",
       "      <td>[3, 79, 29270, 16560, 23379, 63, 23, 11, 1104,...</td>\n",
       "      <td>[3, 44, 1276, 201, 9, 1122, 137, 59, 14, 6, 66...</td>\n",
       "      <td>[3, 79, 29270, 16560, 23379, 63, 23, 11, 1104,...</td>\n",
       "      <td>[3, 44, 1276, 201, 9, 1122, 137, 59, 14, 6, 66...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40802</th>\n",
       "      <td>getting closer to my second goal of  100 a mon...</td>\n",
       "      <td>how old are you  you re likely missing put on ...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], getting, closer, to, my, second, goal,...</td>\n",
       "      <td>[[CLS], how, old, are, you, you, re, likely, m...</td>\n",
       "      <td>[3, 1328, 3158, 9, 657, 180, 1937, 7, 1081, 11...</td>\n",
       "      <td>[3, 283, 2664, 21, 40, 40, 482, 419, 11377, 10...</td>\n",
       "      <td>[3, 1328, 3158, 9, 657, 180, 1937, 7, 1081, 11...</td>\n",
       "      <td>[3, 283, 2664, 21, 40, 40, 482, 419, 11377, 10...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40803</th>\n",
       "      <td>got my dividend portfolio to   10 a month   i ...</td>\n",
       "      <td>as a 30 year old i m kicking myself for not be...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], got, my, dividend, portfolio, to, 10, ...</td>\n",
       "      <td>[[CLS], as, a, 30, year, old, i, m, kick, ##in...</td>\n",
       "      <td>[3, 831, 657, 699, 318, 9, 685, 11, 804, 44, 1...</td>\n",
       "      <td>[3, 18, 11, 1000, 62, 2664, 44, 1276, 7974, 22...</td>\n",
       "      <td>[3, 831, 657, 699, 318, 9, 685, 11, 804, 44, 1...</td>\n",
       "      <td>[3, 18, 11, 1000, 62, 2664, 44, 1276, 7974, 22...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40804</th>\n",
       "      <td>introducing  wsb s first ever paper trading co...</td>\n",
       "      <td>beta</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], introducing, ws, ##b, s, first, ever, ...</td>\n",
       "      <td>[[CLS], beta, [SEP]]</td>\n",
       "      <td>[3, 5638, 7765, 974, 58, 78, 3157, 1659, 429, ...</td>\n",
       "      <td>[3, 5457, 4]</td>\n",
       "      <td>[3, 5638, 7765, 974, 58, 78, 3157, 1659, 429, ...</td>\n",
       "      <td>[3, 5457, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40805</th>\n",
       "      <td>mysterious pneumonia outbreak  overwhelms chin...</td>\n",
       "      <td>covid 23  so puts in like 5 months followed by...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], mys, ##teri, ##ous, pneumo, ##nia, out...</td>\n",
       "      <td>[[CLS], cov, ##id, 23, so, puts, in, like, 5, ...</td>\n",
       "      <td>[3, 22940, 12344, 3837, 21721, 9117, 16267, 76...</td>\n",
       "      <td>[3, 17101, 3531, 2249, 96, 5960, 10, 266, 898,...</td>\n",
       "      <td>[3, 22940, 12344, 3837, 21721, 9117, 16267, 76...</td>\n",
       "      <td>[3, 17101, 3531, 2249, 96, 5960, 10, 266, 898,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40806 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               questions  \\\n",
       "0      my  o position  am i fuk d  i have a severe ad...   \n",
       "1      what s up with market too much red and blood s...   \n",
       "2      earnings drop considerably during or after cen...   \n",
       "3      saudi arabia and opec  makes surprise 1 millio...   \n",
       "4      wework saga cost masayoshi son  11 5 billion a...   \n",
       "...                                                  ...   \n",
       "40801  investment hindsight lessons from a 50 year ol...   \n",
       "40802  getting closer to my second goal of  100 a mon...   \n",
       "40803  got my dividend portfolio to   10 a month   i ...   \n",
       "40804  introducing  wsb s first ever paper trading co...   \n",
       "40805  mysterious pneumonia outbreak  overwhelms chin...   \n",
       "\n",
       "                                                 answers            tags  \\\n",
       "0      yeah you should sell it all now and buy back a...       dividends   \n",
       "1      welcome to r dividends   if you are new to the...       dividends   \n",
       "2                   forward eps instead of actual  skip      stockmarket   \n",
       "3      when we really look at the amount petroleum pr...          stocks   \n",
       "4      wow  they sold nvidia at the bottom in jan 201...          stocks   \n",
       "...                                                  ...             ...   \n",
       "40801  i m going to add one more for the young bucks ...       dividends   \n",
       "40802  how old are you  you re likely missing put on ...       dividends   \n",
       "40803  as a 30 year old i m kicking myself for not be...       dividends   \n",
       "40804                                               beta  wallstreetbets   \n",
       "40805  covid 23  so puts in like 5 months followed by...  wallstreetbets   \n",
       "\n",
       "                                         question_tokens  \\\n",
       "0      [[CLS], my, o, position, am, i, fu, ##k, d, i,...   \n",
       "1      [[CLS], what, s, up, with, market, too, much, ...   \n",
       "2      [[CLS], earnings, drop, considerably, during, ...   \n",
       "3      [[CLS], saudi, arabia, and, opec, makes, surpr...   \n",
       "4      [[CLS], we, ##work, sag, ##a, cost, mas, ##ay,...   \n",
       "...                                                  ...   \n",
       "40801  [[CLS], investment, hind, ##sight, lesson, ##s...   \n",
       "40802  [[CLS], getting, closer, to, my, second, goal,...   \n",
       "40803  [[CLS], got, my, dividend, portfolio, to, 10, ...   \n",
       "40804  [[CLS], introducing, ws, ##b, s, first, ever, ...   \n",
       "40805  [[CLS], mys, ##teri, ##ous, pneumo, ##nia, out...   \n",
       "\n",
       "                                           answer_tokens  \\\n",
       "0      [[CLS], yeah, you, should, sell, it, all, now,...   \n",
       "1      [[CLS], welcome, to, r, dividends, if, you, ar...   \n",
       "2      [[CLS], forward, eps, instead, of, actual, ski...   \n",
       "3      [[CLS], when, we, really, look, at, the, amoun...   \n",
       "4      [[CLS], wo, ##w, they, sold, nvidia, at, the, ...   \n",
       "...                                                  ...   \n",
       "40801  [[CLS], i, m, going, to, add, one, more, for, ...   \n",
       "40802  [[CLS], how, old, are, you, you, re, likely, m...   \n",
       "40803  [[CLS], as, a, 30, year, old, i, m, kick, ##in...   \n",
       "40804                               [[CLS], beta, [SEP]]   \n",
       "40805  [[CLS], cov, ##id, 23, so, puts, in, like, 5, ...   \n",
       "\n",
       "                                            question_ids  \\\n",
       "0      [3, 657, 1506, 497, 1198, 44, 9841, 994, 1514,...   \n",
       "1      [3, 163, 58, 129, 20, 52, 1727, 406, 2932, 8, ...   \n",
       "2      [3, 149, 2535, 6800, 84, 16, 293, 2034, 230, 7...   \n",
       "3      [3, 4807, 6569, 8, 25140, 1221, 5544, 428, 27,...   \n",
       "4      [3, 13, 8724, 21481, 363, 121, 13100, 4345, 29...   \n",
       "...                                                  ...   \n",
       "40801  [3, 79, 29270, 16560, 23379, 63, 23, 11, 1104,...   \n",
       "40802  [3, 1328, 3158, 9, 657, 180, 1937, 7, 1081, 11...   \n",
       "40803  [3, 831, 657, 699, 318, 9, 685, 11, 804, 44, 1...   \n",
       "40804  [3, 5638, 7765, 974, 58, 78, 3157, 1659, 429, ...   \n",
       "40805  [3, 22940, 12344, 3837, 21721, 9117, 16267, 76...   \n",
       "\n",
       "                                              answer_ids  \\\n",
       "0      [3, 15471, 40, 144, 329, 41, 69, 212, 8, 500, ...   \n",
       "1      [3, 2133, 9, 2859, 751, 60, 40, 21, 56, 9, 6, ...   \n",
       "2                  [3, 663, 249, 3592, 7, 499, 26189, 4]   \n",
       "3      [3, 181, 13, 392, 368, 28, 6, 224, 3360, 89, 1...   \n",
       "4      [3, 9566, 1652, 174, 449, 10627, 28, 6, 2169, ...   \n",
       "...                                                  ...   \n",
       "40801  [3, 44, 1276, 201, 9, 1122, 137, 59, 14, 6, 66...   \n",
       "40802  [3, 283, 2664, 21, 40, 40, 482, 419, 11377, 10...   \n",
       "40803  [3, 18, 11, 1000, 62, 2664, 44, 1276, 7974, 22...   \n",
       "40804                                       [3, 5457, 4]   \n",
       "40805  [3, 17101, 3531, 2249, 96, 5960, 10, 266, 898,...   \n",
       "\n",
       "                                        question_ids_pad  \\\n",
       "0      [3, 657, 1506, 497, 1198, 44, 9841, 994, 1514,...   \n",
       "1      [3, 163, 58, 129, 20, 52, 1727, 406, 2932, 8, ...   \n",
       "2      [3, 149, 2535, 6800, 84, 16, 293, 2034, 230, 7...   \n",
       "3      [3, 4807, 6569, 8, 25140, 1221, 5544, 428, 27,...   \n",
       "4      [3, 13, 8724, 21481, 363, 121, 13100, 4345, 29...   \n",
       "...                                                  ...   \n",
       "40801  [3, 79, 29270, 16560, 23379, 63, 23, 11, 1104,...   \n",
       "40802  [3, 1328, 3158, 9, 657, 180, 1937, 7, 1081, 11...   \n",
       "40803  [3, 831, 657, 699, 318, 9, 685, 11, 804, 44, 1...   \n",
       "40804  [3, 5638, 7765, 974, 58, 78, 3157, 1659, 429, ...   \n",
       "40805  [3, 22940, 12344, 3837, 21721, 9117, 16267, 76...   \n",
       "\n",
       "                                          answer_ids_pad  \\\n",
       "0      [3, 15471, 40, 144, 329, 41, 69, 212, 8, 500, ...   \n",
       "1      [3, 2133, 9, 2859, 751, 60, 40, 21, 56, 9, 6, ...   \n",
       "2      [3, 663, 249, 3592, 7, 499, 26189, 4, 0, 0, 0,...   \n",
       "3      [3, 181, 13, 392, 368, 28, 6, 224, 3360, 89, 1...   \n",
       "4      [3, 9566, 1652, 174, 449, 10627, 28, 6, 2169, ...   \n",
       "...                                                  ...   \n",
       "40801  [3, 44, 1276, 201, 9, 1122, 137, 59, 14, 6, 66...   \n",
       "40802  [3, 283, 2664, 21, 40, 40, 482, 419, 11377, 10...   \n",
       "40803  [3, 18, 11, 1000, 62, 2664, 44, 1276, 7974, 22...   \n",
       "40804  [3, 5457, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "40805  [3, 17101, 3531, 2249, 96, 5960, 10, 266, 898,...   \n",
       "\n",
       "                                 question_attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "40801  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "40802  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "40803  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "40804  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n",
       "40805  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                   answer_attention_mask  \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                  ...  \n",
       "40801  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "40802  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "40803  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "40804  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "40805  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[40806 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df_reddit['question_attention_mask'] = qna_df_reddit['question_ids_pad'].apply(lambda x: [1 if i != 0 else 0 for i in x])\n",
    "qna_df_reddit['answer_attention_mask'] = qna_df_reddit['answer_ids_pad'].apply(lambda x: [1 if i != 0 else 0 for i in x])\n",
    "\n",
    "qna_df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qna_df_reddit['question_ids_pad'] = qna_df_reddit['question_ids_pad'].apply(lambda x: x.tolist() if not isinstance(x, list) else x)\n",
    "# qna_df_reddit['answer_ids_pad'] = qna_df_reddit['answer_ids_pad'].apply(lambda x: x.tolist() if not isinstance(x, list) else x)\n",
    "# qna_df_reddit['question_attention_mask'] = qna_df_reddit['question_attention_mask'].apply(lambda x: x.tolist() if not isinstance(x, list) else x)\n",
    "# qna_df_reddit['answer_attention_mask'] = qna_df_reddit['answer_attention_mask'].apply(lambda x: x.tolist() if not isinstance(x, list) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, validation = train_test_split(qna_df_reddit, test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>tags</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>answer_tokens</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>answer_ids</th>\n",
       "      <th>question_ids_pad</th>\n",
       "      <th>answer_ids_pad</th>\n",
       "      <th>question_attention_mask</th>\n",
       "      <th>answer_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27905</th>\n",
       "      <td>insomniac  a top videogame developer s leaks r...</td>\n",
       "      <td>chmod  x men</td>\n",
       "      <td>stocks</td>\n",
       "      <td>[[CLS], inso, ##mni, ##ac, a, top, video, ##ga...</td>\n",
       "      <td>[[CLS], ch, ##mod, x, men, [SEP]]</td>\n",
       "      <td>[3, 17981, 25914, 5197, 11, 909, 1780, 28277, ...</td>\n",
       "      <td>[3, 8551, 15618, 3474, 4640, 4]</td>\n",
       "      <td>[3, 17981, 25914, 5197, 11, 909, 1780, 28277, ...</td>\n",
       "      <td>[3, 8551, 15618, 3474, 4640, 4, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10888</th>\n",
       "      <td>at approximately what account balance did you ...</td>\n",
       "      <td>it depends on how you look at it  i will use o...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], at, approximately, what, account, bala...</td>\n",
       "      <td>[[CLS], it, depends, on, how, you, look, at, i...</td>\n",
       "      <td>[3, 28, 117, 163, 709, 303, 510, 40, 1016, 9, ...</td>\n",
       "      <td>[3, 41, 1686, 19, 283, 40, 368, 28, 41, 44, 36...</td>\n",
       "      <td>[3, 28, 117, 163, 709, 303, 510, 40, 1016, 9, ...</td>\n",
       "      <td>[3, 41, 1686, 19, 283, 40, 368, 28, 41, 44, 36...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11436</th>\n",
       "      <td>an honest 5cents earned this gain is unrivaled...</td>\n",
       "      <td>gains are gains  most new investors blow their...</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], an, honest, 5, ##cent, ##s, earned, th...</td>\n",
       "      <td>[[CLS], gains, are, gains, most, new, investor...</td>\n",
       "      <td>[3, 33, 17848, 898, 8773, 63, 1356, 26, 608, 1...</td>\n",
       "      <td>[3, 596, 21, 596, 242, 56, 311, 12430, 104, 49...</td>\n",
       "      <td>[3, 33, 17848, 898, 8773, 63, 1356, 26, 608, 1...</td>\n",
       "      <td>[3, 596, 21, 596, 242, 56, 311, 12430, 104, 49...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33060</th>\n",
       "      <td>alibaba co founders buy more than  200 million...</td>\n",
       "      <td>pretty sure china wanting to prop up its stock...</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], ali, ##bab, ##a, co, founders, buy, mo...</td>\n",
       "      <td>[[CLS], pretty, sure, china, wanting, to, prop...</td>\n",
       "      <td>[3, 6803, 17284, 363, 3941, 21629, 500, 59, 74...</td>\n",
       "      <td>[3, 1062, 1536, 1308, 17542, 9, 10721, 129, 38...</td>\n",
       "      <td>[3, 6803, 17284, 363, 3941, 21629, 500, 59, 74...</td>\n",
       "      <td>[3, 1062, 1536, 1308, 17542, 9, 10721, 129, 38...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>thanks community  1st of many to come</td>\n",
       "      <td>whoop whoop  i still remember my first   12 fr...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], thanks, community, 1st, of, many, to, ...</td>\n",
       "      <td>[[CLS], who, ##op, who, ##op, i, still, rememb...</td>\n",
       "      <td>[3, 1237, 1968, 8341, 7, 321, 9, 825, 4]</td>\n",
       "      <td>[3, 412, 4567, 412, 4567, 44, 380, 4282, 657, ...</td>\n",
       "      <td>[3, 1237, 1968, 8341, 7, 321, 9, 825, 4, 0, 0,...</td>\n",
       "      <td>[3, 412, 4567, 412, 4567, 44, 380, 4282, 657, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>bank of england warns u s  tech stock valuatio...</td>\n",
       "      <td>they shorted tech stocks to save their economy  s</td>\n",
       "      <td>stocks</td>\n",
       "      <td>[[CLS], bank, of, england, warn, ##s, u, s, te...</td>\n",
       "      <td>[[CLS], they, short, ##ed, tech, stocks, to, s...</td>\n",
       "      <td>[3, 230, 7, 7714, 21798, 63, 4729, 58, 4579, 9...</td>\n",
       "      <td>[3, 174, 1072, 268, 4579, 924, 9, 6177, 104, 1...</td>\n",
       "      <td>[3, 230, 7, 7714, 21798, 63, 4729, 58, 4579, 9...</td>\n",
       "      <td>[3, 174, 1072, 268, 4579, 924, 9, 6177, 104, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>the warped financial logic fueling america s  ...</td>\n",
       "      <td>tl dr     the pandemic boarded up so many stor...</td>\n",
       "      <td>finance</td>\n",
       "      <td>[[CLS], the, war, ##ped, financial, logic, fue...</td>\n",
       "      <td>[[CLS], tl, dr, the, pandemic, board, ##ed, up...</td>\n",
       "      <td>[3, 6, 4661, 7753, 39, 6376, 16729, 723, 58, 1...</td>\n",
       "      <td>[3, 8797, 8836, 6, 16657, 493, 268, 129, 96, 3...</td>\n",
       "      <td>[3, 6, 4661, 7753, 39, 6376, 16729, 723, 58, 1...</td>\n",
       "      <td>[3, 8797, 8836, 6, 16657, 493, 268, 129, 96, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>biden says  banking system is safe  after sili...</td>\n",
       "      <td>is this political inverse cramer  is he tellin...</td>\n",
       "      <td>finance</td>\n",
       "      <td>[[CLS], bid, ##en, says, banking, system, is, ...</td>\n",
       "      <td>[[CLS], is, this, political, inverse, cra, ##m...</td>\n",
       "      <td>[3, 3485, 1781, 8276, 305, 514, 17, 2320, 293,...</td>\n",
       "      <td>[3, 17, 26, 1932, 27154, 6270, 4465, 17, 1418,...</td>\n",
       "      <td>[3, 3485, 1781, 8276, 305, 514, 17, 2320, 293,...</td>\n",
       "      <td>[3, 17, 26, 1932, 27154, 6270, 4465, 17, 1418,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>ai chatbot to replace human order takers at we...</td>\n",
       "      <td>user report                            tota...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], ai, chat, ##bot, to, replace, human, o...</td>\n",
       "      <td>[[CLS], user, report, total, submissions, 2, f...</td>\n",
       "      <td>[3, 7280, 14700, 16253, 9, 2594, 1891, 440, 36...</td>\n",
       "      <td>[3, 2502, 125, 87, 13777, 513, 78, 921, 10, 77...</td>\n",
       "      <td>[3, 7280, 14700, 16253, 9, 2594, 1891, 440, 36...</td>\n",
       "      <td>[3, 2502, 125, 87, 13777, 513, 78, 921, 10, 77...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>semi conductors   ttm revenues vs market cap a...</td>\n",
       "      <td>doesn t matter  semi are running on pure hype ...</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], semi, conduct, ##ors, ttm, revenues, v...</td>\n",
       "      <td>[[CLS], does, ##n, t, matter, semi, are, runni...</td>\n",
       "      <td>[3, 7826, 1206, 4508, 22895, 145, 4364, 52, 17...</td>\n",
       "      <td>[3, 262, 445, 599, 2810, 7826, 21, 2376, 19, 6...</td>\n",
       "      <td>[3, 7826, 1206, 4508, 22895, 145, 4364, 52, 17...</td>\n",
       "      <td>[3, 262, 445, 599, 2810, 7826, 21, 2376, 19, 6...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32644 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               questions  \\\n",
       "27905  insomniac  a top videogame developer s leaks r...   \n",
       "10888  at approximately what account balance did you ...   \n",
       "11436  an honest 5cents earned this gain is unrivaled...   \n",
       "33060  alibaba co founders buy more than  200 million...   \n",
       "2752           thanks community  1st of many to come       \n",
       "...                                                  ...   \n",
       "6265   bank of england warns u s  tech stock valuatio...   \n",
       "11284  the warped financial logic fueling america s  ...   \n",
       "38158  biden says  banking system is safe  after sili...   \n",
       "860    ai chatbot to replace human order takers at we...   \n",
       "15795  semi conductors   ttm revenues vs market cap a...   \n",
       "\n",
       "                                                 answers            tags  \\\n",
       "27905                                      chmod  x men           stocks   \n",
       "10888  it depends on how you look at it  i will use o...       dividends   \n",
       "11436  gains are gains  most new investors blow their...     stockmarket   \n",
       "33060  pretty sure china wanting to prop up its stock...     stockmarket   \n",
       "2752   whoop whoop  i still remember my first   12 fr...       dividends   \n",
       "...                                                  ...             ...   \n",
       "6265   they shorted tech stocks to save their economy  s          stocks   \n",
       "11284  tl dr     the pandemic boarded up so many stor...         finance   \n",
       "38158  is this political inverse cramer  is he tellin...         finance   \n",
       "860       user report                            tota...  wallstreetbets   \n",
       "15795  doesn t matter  semi are running on pure hype ...     stockmarket   \n",
       "\n",
       "                                         question_tokens  \\\n",
       "27905  [[CLS], inso, ##mni, ##ac, a, top, video, ##ga...   \n",
       "10888  [[CLS], at, approximately, what, account, bala...   \n",
       "11436  [[CLS], an, honest, 5, ##cent, ##s, earned, th...   \n",
       "33060  [[CLS], ali, ##bab, ##a, co, founders, buy, mo...   \n",
       "2752   [[CLS], thanks, community, 1st, of, many, to, ...   \n",
       "...                                                  ...   \n",
       "6265   [[CLS], bank, of, england, warn, ##s, u, s, te...   \n",
       "11284  [[CLS], the, war, ##ped, financial, logic, fue...   \n",
       "38158  [[CLS], bid, ##en, says, banking, system, is, ...   \n",
       "860    [[CLS], ai, chat, ##bot, to, replace, human, o...   \n",
       "15795  [[CLS], semi, conduct, ##ors, ttm, revenues, v...   \n",
       "\n",
       "                                           answer_tokens  \\\n",
       "27905                  [[CLS], ch, ##mod, x, men, [SEP]]   \n",
       "10888  [[CLS], it, depends, on, how, you, look, at, i...   \n",
       "11436  [[CLS], gains, are, gains, most, new, investor...   \n",
       "33060  [[CLS], pretty, sure, china, wanting, to, prop...   \n",
       "2752   [[CLS], who, ##op, who, ##op, i, still, rememb...   \n",
       "...                                                  ...   \n",
       "6265   [[CLS], they, short, ##ed, tech, stocks, to, s...   \n",
       "11284  [[CLS], tl, dr, the, pandemic, board, ##ed, up...   \n",
       "38158  [[CLS], is, this, political, inverse, cra, ##m...   \n",
       "860    [[CLS], user, report, total, submissions, 2, f...   \n",
       "15795  [[CLS], does, ##n, t, matter, semi, are, runni...   \n",
       "\n",
       "                                            question_ids  \\\n",
       "27905  [3, 17981, 25914, 5197, 11, 909, 1780, 28277, ...   \n",
       "10888  [3, 28, 117, 163, 709, 303, 510, 40, 1016, 9, ...   \n",
       "11436  [3, 33, 17848, 898, 8773, 63, 1356, 26, 608, 1...   \n",
       "33060  [3, 6803, 17284, 363, 3941, 21629, 500, 59, 74...   \n",
       "2752            [3, 1237, 1968, 8341, 7, 321, 9, 825, 4]   \n",
       "...                                                  ...   \n",
       "6265   [3, 230, 7, 7714, 21798, 63, 4729, 58, 4579, 9...   \n",
       "11284  [3, 6, 4661, 7753, 39, 6376, 16729, 723, 58, 1...   \n",
       "38158  [3, 3485, 1781, 8276, 305, 514, 17, 2320, 293,...   \n",
       "860    [3, 7280, 14700, 16253, 9, 2594, 1891, 440, 36...   \n",
       "15795  [3, 7826, 1206, 4508, 22895, 145, 4364, 52, 17...   \n",
       "\n",
       "                                              answer_ids  \\\n",
       "27905                    [3, 8551, 15618, 3474, 4640, 4]   \n",
       "10888  [3, 41, 1686, 19, 283, 40, 368, 28, 41, 44, 36...   \n",
       "11436  [3, 596, 21, 596, 242, 56, 311, 12430, 104, 49...   \n",
       "33060  [3, 1062, 1536, 1308, 17542, 9, 10721, 129, 38...   \n",
       "2752   [3, 412, 4567, 412, 4567, 44, 380, 4282, 657, ...   \n",
       "...                                                  ...   \n",
       "6265   [3, 174, 1072, 268, 4579, 924, 9, 6177, 104, 1...   \n",
       "11284  [3, 8797, 8836, 6, 16657, 493, 268, 129, 96, 3...   \n",
       "38158  [3, 17, 26, 1932, 27154, 6270, 4465, 17, 1418,...   \n",
       "860    [3, 2502, 125, 87, 13777, 513, 78, 921, 10, 77...   \n",
       "15795  [3, 262, 445, 599, 2810, 7826, 21, 2376, 19, 6...   \n",
       "\n",
       "                                        question_ids_pad  \\\n",
       "27905  [3, 17981, 25914, 5197, 11, 909, 1780, 28277, ...   \n",
       "10888  [3, 28, 117, 163, 709, 303, 510, 40, 1016, 9, ...   \n",
       "11436  [3, 33, 17848, 898, 8773, 63, 1356, 26, 608, 1...   \n",
       "33060  [3, 6803, 17284, 363, 3941, 21629, 500, 59, 74...   \n",
       "2752   [3, 1237, 1968, 8341, 7, 321, 9, 825, 4, 0, 0,...   \n",
       "...                                                  ...   \n",
       "6265   [3, 230, 7, 7714, 21798, 63, 4729, 58, 4579, 9...   \n",
       "11284  [3, 6, 4661, 7753, 39, 6376, 16729, 723, 58, 1...   \n",
       "38158  [3, 3485, 1781, 8276, 305, 514, 17, 2320, 293,...   \n",
       "860    [3, 7280, 14700, 16253, 9, 2594, 1891, 440, 36...   \n",
       "15795  [3, 7826, 1206, 4508, 22895, 145, 4364, 52, 17...   \n",
       "\n",
       "                                          answer_ids_pad  \\\n",
       "27905  [3, 8551, 15618, 3474, 4640, 4, 0, 0, 0, 0, 0,...   \n",
       "10888  [3, 41, 1686, 19, 283, 40, 368, 28, 41, 44, 36...   \n",
       "11436  [3, 596, 21, 596, 242, 56, 311, 12430, 104, 49...   \n",
       "33060  [3, 1062, 1536, 1308, 17542, 9, 10721, 129, 38...   \n",
       "2752   [3, 412, 4567, 412, 4567, 44, 380, 4282, 657, ...   \n",
       "...                                                  ...   \n",
       "6265   [3, 174, 1072, 268, 4579, 924, 9, 6177, 104, 1...   \n",
       "11284  [3, 8797, 8836, 6, 16657, 493, 268, 129, 96, 3...   \n",
       "38158  [3, 17, 26, 1932, 27154, 6270, 4465, 17, 1418,...   \n",
       "860    [3, 2502, 125, 87, 13777, 513, 78, 921, 10, 77...   \n",
       "15795  [3, 262, 445, 599, 2810, 7826, 21, 2376, 19, 6...   \n",
       "\n",
       "                                 question_attention_mask  \\\n",
       "27905  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "10888  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "11436  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "33060  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2752   [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "6265   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "11284  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "38158  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
       "860    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "15795  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                   answer_attention_mask  \n",
       "27905  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10888  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "11436  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "33060  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2752   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  \n",
       "...                                                  ...  \n",
       "6265   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...  \n",
       "11284  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "38158  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "860    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "15795  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[32644 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>tags</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>answer_tokens</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>answer_ids</th>\n",
       "      <th>question_ids_pad</th>\n",
       "      <th>answer_ids_pad</th>\n",
       "      <th>question_attention_mask</th>\n",
       "      <th>answer_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35243</th>\n",
       "      <td>what s the difference between now and the grea...</td>\n",
       "      <td>2008 was primarily a real estate crash because...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], what, s, the, difference, between, now...</td>\n",
       "      <td>[[CLS], 2008, was, primarily, a, real, estate,...</td>\n",
       "      <td>[3, 163, 58, 6, 1748, 335, 212, 8, 6, 1080, 39...</td>\n",
       "      <td>[3, 519, 35, 106, 11, 373, 558, 6270, 3481, 23...</td>\n",
       "      <td>[3, 163, 58, 6, 1748, 335, 212, 8, 6, 1080, 39...</td>\n",
       "      <td>[3, 519, 35, 106, 11, 373, 558, 6270, 3481, 23...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30292</th>\n",
       "      <td>calls or puts on  tsla</td>\n",
       "      <td>1  stock go up  elon future gas station overlo...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], calls, or, puts, on, ts, ##la, [SEP]]</td>\n",
       "      <td>[[CLS], 1, stock, go, up, elon, future, gas, s...</td>\n",
       "      <td>[3, 2602, 16, 5960, 19, 5796, 3463, 4]</td>\n",
       "      <td>[3, 428, 93, 478, 129, 29620, 128, 297, 3826, ...</td>\n",
       "      <td>[3, 2602, 16, 5960, 19, 5796, 3463, 4, 0, 0, 0...</td>\n",
       "      <td>[3, 428, 93, 478, 129, 29620, 128, 297, 3826, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10423</th>\n",
       "      <td>dividends in a regular brokerage account  many...</td>\n",
       "      <td>i get taxed 30  on all us stock dividends   ze...</td>\n",
       "      <td>dividends</td>\n",
       "      <td>[[CLS], dividends, in, a, regular, brokerage, ...</td>\n",
       "      <td>[[CLS], i, get, taxed, 30, on, all, us, stock,...</td>\n",
       "      <td>[3, 751, 10, 11, 2749, 2267, 709, 321, 1043, 4...</td>\n",
       "      <td>[3, 44, 391, 7939, 1000, 19, 69, 90, 93, 751, ...</td>\n",
       "      <td>[3, 751, 10, 11, 2749, 2267, 709, 321, 1043, 4...</td>\n",
       "      <td>[3, 44, 391, 7939, 1000, 19, 69, 90, 93, 751, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29321</th>\n",
       "      <td>what ceo scott mcnealy had to say to investors...</td>\n",
       "      <td>there is a reason that someone said  idiot bor...</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], what, ceo, scott, mcn, ##eal, ##y, had...</td>\n",
       "      <td>[[CLS], there, is, a, reason, that, someone, s...</td>\n",
       "      <td>[3, 163, 2303, 2263, 17679, 24213, 387, 159, 9...</td>\n",
       "      <td>[3, 112, 17, 11, 2040, 15, 8741, 960, 20784, 5...</td>\n",
       "      <td>[3, 163, 2303, 2263, 17679, 24213, 387, 159, 9...</td>\n",
       "      <td>[3, 112, 17, 11, 2040, 15, 8741, 960, 20784, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25858</th>\n",
       "      <td>telesat stock surges 50  after satellite inter...</td>\n",
       "      <td>how long were they overpaying their suppliers ...</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], telesat, stock, surge, ##s, 50, after,...</td>\n",
       "      <td>[[CLS], how, long, were, they, overp, ##ay, ##...</td>\n",
       "      <td>[3, 22261, 93, 9586, 63, 1104, 293, 2894, 1178...</td>\n",
       "      <td>[3, 283, 637, 57, 174, 22927, 4345, 223, 104, ...</td>\n",
       "      <td>[3, 22261, 93, 9586, 63, 1104, 293, 2894, 1178...</td>\n",
       "      <td>[3, 283, 637, 57, 174, 22927, 4345, 223, 104, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7845</th>\n",
       "      <td>when you read headlines like this</td>\n",
       "      <td>izbrisano</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], when, you, read, headlines, like, this...</td>\n",
       "      <td>[[CLS], i, ##zb, ##ris, ##ano, [SEP]]</td>\n",
       "      <td>[3, 181, 40, 1606, 21608, 266, 26, 4]</td>\n",
       "      <td>[3, 44, 21596, 6632, 10225, 4]</td>\n",
       "      <td>[3, 181, 40, 1606, 21608, 266, 26, 4, 0, 0, 0,...</td>\n",
       "      <td>[3, 44, 21596, 6632, 10225, 4, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6194</th>\n",
       "      <td>absolute legend  inverse cramer strikes yet ag...</td>\n",
       "      <td>for someone as idiot as him  or clearly bad fa...</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>[[CLS], absolute, legend, inverse, cra, ##mer,...</td>\n",
       "      <td>[[CLS], for, someone, as, idi, ##ot, as, him, ...</td>\n",
       "      <td>[3, 2241, 10866, 27154, 6270, 4465, 12206, 143...</td>\n",
       "      <td>[3, 14, 8741, 18, 20784, 5536, 18, 6916, 16, 1...</td>\n",
       "      <td>[3, 2241, 10866, 27154, 6270, 4465, 12206, 143...</td>\n",
       "      <td>[3, 14, 8741, 18, 20784, 5536, 18, 6916, 16, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>deutsche bank kiss of death</td>\n",
       "      <td>user report                            tota...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], deutsche, bank, kis, ##s, of, death, [...</td>\n",
       "      <td>[[CLS], user, report, total, submissions, 5, f...</td>\n",
       "      <td>[3, 1069, 230, 21377, 63, 7, 6539, 4]</td>\n",
       "      <td>[3, 2502, 125, 87, 13777, 898, 78, 921, 10, 77...</td>\n",
       "      <td>[3, 1069, 230, 21377, 63, 7, 6539, 4, 0, 0, 0,...</td>\n",
       "      <td>[3, 2502, 125, 87, 13777, 898, 78, 921, 10, 77...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35460</th>\n",
       "      <td>all in on reddit calls</td>\n",
       "      <td>i thought for sure puts  but based on how ever...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[[CLS], all, in, on, redd, ##it, calls, [SEP]]</td>\n",
       "      <td>[[CLS], i, thought, for, sure, puts, but, base...</td>\n",
       "      <td>[3, 69, 10, 19, 29677, 3183, 2602, 4]</td>\n",
       "      <td>[3, 44, 2549, 14, 1536, 5960, 71, 114, 19, 283...</td>\n",
       "      <td>[3, 69, 10, 19, 29677, 3183, 2602, 4, 0, 0, 0,...</td>\n",
       "      <td>[3, 44, 2549, 14, 1536, 5960, 71, 114, 19, 283...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>friend reported me insider trading solicitatio...</td>\n",
       "      <td>i m buying puts on your friendship</td>\n",
       "      <td>stocks</td>\n",
       "      <td>[[CLS], frie, ##nd, reported, me, insider, tra...</td>\n",
       "      <td>[[CLS], i, m, buying, puts, on, your, friends,...</td>\n",
       "      <td>[3, 25058, 4121, 330, 914, 9076, 429, 1524, 43...</td>\n",
       "      <td>[3, 44, 1276, 2180, 5960, 19, 185, 17950, 6139...</td>\n",
       "      <td>[3, 25058, 4121, 330, 914, 9076, 429, 1524, 43...</td>\n",
       "      <td>[3, 44, 1276, 2180, 5960, 19, 185, 17950, 6139...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8162 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               questions  \\\n",
       "35243  what s the difference between now and the grea...   \n",
       "30292                           calls or puts on  tsla     \n",
       "10423  dividends in a regular brokerage account  many...   \n",
       "29321  what ceo scott mcnealy had to say to investors...   \n",
       "25858  telesat stock surges 50  after satellite inter...   \n",
       "...                                                  ...   \n",
       "7845                  when you read headlines like this    \n",
       "6194   absolute legend  inverse cramer strikes yet ag...   \n",
       "7605                        deutsche bank kiss of death    \n",
       "35460                            all in on reddit calls    \n",
       "15602  friend reported me insider trading solicitatio...   \n",
       "\n",
       "                                                 answers            tags  \\\n",
       "35243  2008 was primarily a real estate crash because...       dividends   \n",
       "30292  1  stock go up  elon future gas station overlo...  wallstreetbets   \n",
       "10423  i get taxed 30  on all us stock dividends   ze...       dividends   \n",
       "29321  there is a reason that someone said  idiot bor...     stockmarket   \n",
       "25858  how long were they overpaying their suppliers ...     stockmarket   \n",
       "...                                                  ...             ...   \n",
       "7845                                          izbrisano   wallstreetbets   \n",
       "6194   for someone as idiot as him  or clearly bad fa...     stockmarket   \n",
       "7605      user report                            tota...  wallstreetbets   \n",
       "35460  i thought for sure puts  but based on how ever...  wallstreetbets   \n",
       "15602                i m buying puts on your friendship           stocks   \n",
       "\n",
       "                                         question_tokens  \\\n",
       "35243  [[CLS], what, s, the, difference, between, now...   \n",
       "30292      [[CLS], calls, or, puts, on, ts, ##la, [SEP]]   \n",
       "10423  [[CLS], dividends, in, a, regular, brokerage, ...   \n",
       "29321  [[CLS], what, ceo, scott, mcn, ##eal, ##y, had...   \n",
       "25858  [[CLS], telesat, stock, surge, ##s, 50, after,...   \n",
       "...                                                  ...   \n",
       "7845   [[CLS], when, you, read, headlines, like, this...   \n",
       "6194   [[CLS], absolute, legend, inverse, cra, ##mer,...   \n",
       "7605   [[CLS], deutsche, bank, kis, ##s, of, death, [...   \n",
       "35460     [[CLS], all, in, on, redd, ##it, calls, [SEP]]   \n",
       "15602  [[CLS], frie, ##nd, reported, me, insider, tra...   \n",
       "\n",
       "                                           answer_tokens  \\\n",
       "35243  [[CLS], 2008, was, primarily, a, real, estate,...   \n",
       "30292  [[CLS], 1, stock, go, up, elon, future, gas, s...   \n",
       "10423  [[CLS], i, get, taxed, 30, on, all, us, stock,...   \n",
       "29321  [[CLS], there, is, a, reason, that, someone, s...   \n",
       "25858  [[CLS], how, long, were, they, overp, ##ay, ##...   \n",
       "...                                                  ...   \n",
       "7845               [[CLS], i, ##zb, ##ris, ##ano, [SEP]]   \n",
       "6194   [[CLS], for, someone, as, idi, ##ot, as, him, ...   \n",
       "7605   [[CLS], user, report, total, submissions, 5, f...   \n",
       "35460  [[CLS], i, thought, for, sure, puts, but, base...   \n",
       "15602  [[CLS], i, m, buying, puts, on, your, friends,...   \n",
       "\n",
       "                                            question_ids  \\\n",
       "35243  [3, 163, 58, 6, 1748, 335, 212, 8, 6, 1080, 39...   \n",
       "30292             [3, 2602, 16, 5960, 19, 5796, 3463, 4]   \n",
       "10423  [3, 751, 10, 11, 2749, 2267, 709, 321, 1043, 4...   \n",
       "29321  [3, 163, 2303, 2263, 17679, 24213, 387, 159, 9...   \n",
       "25858  [3, 22261, 93, 9586, 63, 1104, 293, 2894, 1178...   \n",
       "...                                                  ...   \n",
       "7845               [3, 181, 40, 1606, 21608, 266, 26, 4]   \n",
       "6194   [3, 2241, 10866, 27154, 6270, 4465, 12206, 143...   \n",
       "7605               [3, 1069, 230, 21377, 63, 7, 6539, 4]   \n",
       "35460              [3, 69, 10, 19, 29677, 3183, 2602, 4]   \n",
       "15602  [3, 25058, 4121, 330, 914, 9076, 429, 1524, 43...   \n",
       "\n",
       "                                              answer_ids  \\\n",
       "35243  [3, 519, 35, 106, 11, 373, 558, 6270, 3481, 23...   \n",
       "30292  [3, 428, 93, 478, 129, 29620, 128, 297, 3826, ...   \n",
       "10423  [3, 44, 391, 7939, 1000, 19, 69, 90, 93, 751, ...   \n",
       "29321  [3, 112, 17, 11, 2040, 15, 8741, 960, 20784, 5...   \n",
       "25858  [3, 283, 637, 57, 174, 22927, 4345, 223, 104, ...   \n",
       "...                                                  ...   \n",
       "7845                      [3, 44, 21596, 6632, 10225, 4]   \n",
       "6194   [3, 14, 8741, 18, 20784, 5536, 18, 6916, 16, 1...   \n",
       "7605   [3, 2502, 125, 87, 13777, 898, 78, 921, 10, 77...   \n",
       "35460  [3, 44, 2549, 14, 1536, 5960, 71, 114, 19, 283...   \n",
       "15602  [3, 44, 1276, 2180, 5960, 19, 185, 17950, 6139...   \n",
       "\n",
       "                                        question_ids_pad  \\\n",
       "35243  [3, 163, 58, 6, 1748, 335, 212, 8, 6, 1080, 39...   \n",
       "30292  [3, 2602, 16, 5960, 19, 5796, 3463, 4, 0, 0, 0...   \n",
       "10423  [3, 751, 10, 11, 2749, 2267, 709, 321, 1043, 4...   \n",
       "29321  [3, 163, 2303, 2263, 17679, 24213, 387, 159, 9...   \n",
       "25858  [3, 22261, 93, 9586, 63, 1104, 293, 2894, 1178...   \n",
       "...                                                  ...   \n",
       "7845   [3, 181, 40, 1606, 21608, 266, 26, 4, 0, 0, 0,...   \n",
       "6194   [3, 2241, 10866, 27154, 6270, 4465, 12206, 143...   \n",
       "7605   [3, 1069, 230, 21377, 63, 7, 6539, 4, 0, 0, 0,...   \n",
       "35460  [3, 69, 10, 19, 29677, 3183, 2602, 4, 0, 0, 0,...   \n",
       "15602  [3, 25058, 4121, 330, 914, 9076, 429, 1524, 43...   \n",
       "\n",
       "                                          answer_ids_pad  \\\n",
       "35243  [3, 519, 35, 106, 11, 373, 558, 6270, 3481, 23...   \n",
       "30292  [3, 428, 93, 478, 129, 29620, 128, 297, 3826, ...   \n",
       "10423  [3, 44, 391, 7939, 1000, 19, 69, 90, 93, 751, ...   \n",
       "29321  [3, 112, 17, 11, 2040, 15, 8741, 960, 20784, 5...   \n",
       "25858  [3, 283, 637, 57, 174, 22927, 4345, 223, 104, ...   \n",
       "...                                                  ...   \n",
       "7845   [3, 44, 21596, 6632, 10225, 4, 0, 0, 0, 0, 0, ...   \n",
       "6194   [3, 14, 8741, 18, 20784, 5536, 18, 6916, 16, 1...   \n",
       "7605   [3, 2502, 125, 87, 13777, 898, 78, 921, 10, 77...   \n",
       "35460  [3, 44, 2549, 14, 1536, 5960, 71, 114, 19, 283...   \n",
       "15602  [3, 44, 1276, 2180, 5960, 19, 185, 17950, 6139...   \n",
       "\n",
       "                                 question_attention_mask  \\\n",
       "35243  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "30292  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10423  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "29321  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "25858  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "7845   [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6194   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
       "7605   [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "35460  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15602  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                   answer_attention_mask  \n",
       "35243  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "30292  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "10423  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "29321  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
       "25858  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                  ...  \n",
       "7845   [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6194   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "7605   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "35460  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "15602  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[8162 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.DataFrame(train)\n",
    "# validation = pd.DataFrame(validation)\n",
    "# train.to_csv('train_data_chatbot.csv',index=False)\n",
    "# validation.to_csv('validation_data_chatbot.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109751808 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,751,808\n",
      "Trainable params: 109,751,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specify the GPU device\n",
    "device = '/GPU:0'\n",
    "\n",
    "# Create a strategy to run on the GPU device\n",
    "strategy = tf.distribute.OneDeviceStrategy(device)\n",
    "\n",
    "# Run the model within the strategy's scope\n",
    "with strategy.scope():\n",
    "    finbert_model = TFAutoModel.from_pretrained(\"FinanceInc/finbert-pretrain\", from_pt=True)\n",
    "    finbert_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "# # Load the GPT-2 model\n",
    "# gpt2_model = TFGPT2LMHeadModel.from_pretrained(\"FinanceInc/finbert-pretrain\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train #pd.read_csv('train_data_chatbot.csv')\n",
    "\n",
    "question_ids_pad = train_data['question_ids_pad'].tolist()\n",
    "answer_ids_pad = train_data['answer_ids_pad'].tolist()\n",
    "question_attention_mask = train_data['question_attention_mask'].tolist()\n",
    "answer_attention_mask = train_data['answer_attention_mask'].tolist()\n",
    "\n",
    "\n",
    "question_ids_pad = tf.convert_to_tensor(question_ids_pad)\n",
    "answer_ids_pad = tf.convert_to_tensor(answer_ids_pad)\n",
    "question_attention_mask = tf.convert_to_tensor(question_attention_mask)\n",
    "answer_attention_mask = tf.convert_to_tensor(answer_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    3 17981 25914 ...     0     0     0]\n",
      " [    3    28   117 ...     0     0     0]], shape=(2, 5951), dtype=int32)\n",
      "(32644, 5951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 00:59:20.028141: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (mklcpu) ran out of memory trying to allocate 555.79GiB (rounded to 596780371968)requested by op ResourceGather\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-01-29 00:59:20.028191: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for mklcpu\n",
      "2024-01-29 00:59:20.028218: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028228: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028235: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028241: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028247: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028254: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028260: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028266: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028273: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028279: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028285: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028292: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028298: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028307: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 3.00MiB client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028314: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 4.50MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028321: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028327: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028333: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028342: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 3, Chunks in use: 2. 289.34MiB allocated for chunks. 180.90MiB in use in bin. 180.90MiB client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028350: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 4, Chunks in use: 0. 587.04MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028360: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 9, Chunks in use: 6. 7.13GiB allocated for chunks. 4.34GiB in use in bin. 4.34GiB client-requested in use in bin.\n",
      "2024-01-29 00:59:20.028369: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 555.79GiB was 256.00MiB, Chunk State: \n",
      "2024-01-29 00:59:20.028382: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 421.55MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 90.45MiB | Requested Size: 90.45MiB | in_use: 1 | bin_num: -1\n",
      "2024-01-29 00:59:20.028391: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 565.88MiB | Requested Size: 9.00MiB | in_use: 0 | bin_num: 20, prev:   Size: 741.06MiB | Requested Size: 741.06MiB | in_use: 1 | bin_num: -1\n",
      "2024-01-29 00:59:20.028400: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 1.83GiB | Requested Size: 2.25MiB | in_use: 0 | bin_num: 20, prev:   Size: 741.06MiB | Requested Size: 741.06MiB | in_use: 1 | bin_num: -1\n",
      "2024-01-29 00:59:20.028405: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 4294967296\n",
      "2024-01-29 00:59:20.028413: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f487bffb040 of size 777057792 next 4\n",
      "2024-01-29 00:59:20.028419: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f48aa50a640 of size 777057792 next 74\n",
      "2024-01-29 00:59:20.028424: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f48d8a19c40 of size 777057792 next 83\n",
      "2024-01-29 00:59:20.028429: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4906f29240 of size 1963793920 next 18446744073709551615\n",
      "2024-01-29 00:59:20.028435: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 2147483648\n",
      "2024-01-29 00:59:20.028440: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f497fffd040 of size 777057792 next 78\n",
      "2024-01-29 00:59:20.028445: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f49ae50c640 of size 777057792 next 76\n",
      "2024-01-29 00:59:20.028450: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f49dca1bc40 of size 593368064 next 18446744073709551615\n",
      "2024-01-29 00:59:20.028455: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 1073741824\n",
      "2024-01-29 00:59:20.028460: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f49ffffe040 of size 777057792 next 79\n",
      "2024-01-29 00:59:20.028466: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f4a2e50d640 of size 94841856 next 91\n",
      "2024-01-29 00:59:20.028471: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4a33f80240 of size 201842176 next 18446744073709551615\n",
      "2024-01-29 00:59:20.028476: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 536870912\n",
      "2024-01-29 00:59:20.028481: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f4a3ffff040 of size 94841856 next 81\n",
      "2024-01-29 00:59:20.028486: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4a45a71c40 of size 442029056 next 18446744073709551615\n",
      "2024-01-29 00:59:20.028491: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 268435456\n",
      "2024-01-29 00:59:20.028496: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4af0186040 of size 4718592 next 66\n",
      "2024-01-29 00:59:20.028502: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f4af0606040 of size 2359296 next 75\n",
      "2024-01-29 00:59:20.028509: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4af0846040 of size 113716224 next 69\n",
      "2024-01-29 00:59:20.028514: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f4af74b8c40 of size 2359296 next 64\n",
      "2024-01-29 00:59:20.028519: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4af76f8c40 of size 145282048 next 18446744073709551615\n",
      "2024-01-29 00:59:20.028524: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 134217728\n",
      "2024-01-29 00:59:20.028531: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4b00187040 of size 134217728 next 18446744073709551615\n",
      "2024-01-29 00:59:20.028536: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 134217728\n",
      "2024-01-29 00:59:20.028542: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f4b08188040 of size 134217728 next 18446744073709551615\n",
      "2024-01-29 00:59:20.028547: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2024-01-29 00:59:20.028553: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2359296 totalling 4.50MiB\n",
      "2024-01-29 00:59:20.028560: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 94841856 totalling 180.90MiB\n",
      "2024-01-29 00:59:20.028566: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 6 Chunks of size 777057792 totalling 4.34GiB\n",
      "2024-01-29 00:59:20.028572: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 4.52GiB\n",
      "2024-01-29 00:59:20.028577: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 8589934592 memory_limit_: 257923256320 available bytes: 249333321728 curr_region_allocation_bytes_: 8589934592\n",
      "2024-01-29 00:59:20.028587: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                    257923256320\n",
      "InUse:                      4856749056\n",
      "MaxInUse:                   5058591232\n",
      "NumAllocs:                         462\n",
      "MaxAllocSize:                777057792\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-29 00:59:20.028596: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************______________________*******************______***********_**____*_*____\n",
      "2024-01-29 00:59:20.028620: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at resource_variable_ops.cc:726 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32644,5951,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[32644,5951,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:ResourceGather]\n\nCall arguments received by layer 'embeddings' (type TFBertEmbeddings):\n  • input_ids=tf.Tensor(shape=(32644, 5951), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(32644, 5951), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m question_ids_pad \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(question_ids_pad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(question_ids_pad\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m encoded_data \u001b[38;5;241m=\u001b[39m \u001b[43mfinbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_ids_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/nlp_env/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda/envs/nlp_env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/nlp_env/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:1088\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1067\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda/envs/nlp_env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/nlp_env/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:780\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill(dims\u001b[38;5;241m=\u001b[39minput_shape, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 780\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m attention_mask_shape \u001b[38;5;241m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[0;32m~/anaconda/envs/nlp_env/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:203\u001b[0m, in \u001b[0;36mTFBertEmbeddings.call\u001b[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     check_embeddings_within_bounds(input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m--> 203\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m shape_list(inputs_embeds)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[32644,5951,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:ResourceGather]\n\nCall arguments received by layer 'embeddings' (type TFBertEmbeddings):\n  • input_ids=tf.Tensor(shape=(32644, 5951), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(32644, 5951), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False"
     ]
    }
   ],
   "source": [
    "print(question_ids_pad[:2])\n",
    "question_ids_pad = tf.concat(question_ids_pad, axis=0)\n",
    "print(question_ids_pad.shape)\n",
    "\n",
    "#encode data for every 1000 rows\n",
    "for i in range(0, len(question_ids_pad), 1000):\n",
    "    encoded_data = finbert_model(question_ids_pad[i:i+999], attention_mask=question_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decode the encoded data using the gpt2_model\n",
    "# decoded_data = gpt2_model.generate(input_ids=encoded_data['last_hidden_state'], attention_mask=answer_attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
